{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "00001-485154dd-4c90-4279-9b96-1073e3409f62",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "00001-c3414d0a-d791-409a-9500-09210922f27c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from aman_test_i3d4 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "00003-37c31998-9bb3-41e3-beb6-3a902beed143",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('datasets.pkl', 'rb') as f:\n",
    "   datasets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_id": "00001-20a4299a-14d7-44ba-a7ab-f4ee04971ecc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "mode = 'rgb'\n",
    "num_classes = 2000\n",
    "# save_model = './checkpoints/'\n",
    "\n",
    " \n",
    "## Change to where the videos are located\n",
    "root = {'word':'videos'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "00002-ce1c6f3c-c61e-4ac2-8884-cb2fd84b3b51",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_split = 'preprocess/nslt_2000.json'\n",
    "\n",
    "weights = 'checkpoints/nslt_2000_065846_0.447803.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_id": "00003-8275ba65-d980-445a-8d9f-6984b4654d41",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 2845 30173 0.000000 0.000351 0.000351\n",
      "videos\\24028.mp4 could not be read for some reason.  Skipping\n",
      "2 / 2845 24028 0.000000 0.000351 0.000351\n",
      "videos\\38843.mp4 could not be read for some reason.  Skipping\n",
      "3 / 2845 38843 0.000000 0.000351 0.000351\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m run(mode\u001b[38;5;241m=\u001b[39mmode, root\u001b[38;5;241m=\u001b[39mroot, train_split\u001b[38;5;241m=\u001b[39mtrain_split, weights\u001b[38;5;241m=\u001b[39mweights, datasets\u001b[38;5;241m=\u001b[39mdatasets, num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n",
      "File \u001b[1;32mD:\\DL_Project\\wlasl-baseline-model\\wlasl-baseline-model\\aman_test_i3d4.py:95\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(mode, root, train_split, datasets, weights, num_classes)\u001b[0m\n\u001b[0;32m     92\u001b[0m skipped_videos \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Evaluation loop\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(val_dataloader, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     inputs, labels, video_id \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     98\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\DL_Project\\wlasl-baseline-model\\wlasl-baseline-model\\datasets\\nslt_dataset.py:211\u001b[0m, in \u001b[0;36mNSLT.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    206\u001b[0m     start_f \u001b[38;5;241m=\u001b[39m start_frame\n\u001b[0;32m    208\u001b[0m \u001b[38;5;66;03m# From this start frame, grab 64 consecutive frames\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# and make sure we normalize the frames from [-1, 1]\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Output should be 64 x 256 x 256 x 3\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m imgs \u001b[38;5;241m=\u001b[39m load_rgb_frames_from_video(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m], vid, start_f, total_frames)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# If we can't load this video for some reason, signal a skip\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m imgs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mD:\\DL_Project\\wlasl-baseline-model\\wlasl-baseline-model\\datasets\\nslt_dataset.py:46\u001b[0m, in \u001b[0;36mload_rgb_frames_from_video\u001b[1;34m(vid_root, vid, start, num, resize)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_rgb_frames_from_video\u001b[39m(vid_root, vid, start, num, resize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)):\n\u001b[0;32m     44\u001b[0m     video_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(vid_root, vid \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m     vidcap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(video_path)\n\u001b[0;32m     48\u001b[0m     frames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     50\u001b[0m     total_frames \u001b[38;5;241m=\u001b[39m vidcap\u001b[38;5;241m.\u001b[39mget(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_FRAME_COUNT)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run(mode=mode, root=root, train_split=train_split, weights=weights, datasets=datasets, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "510214f7-770d-40ec-8cb3-6dfc55c766b9",
  "kernelspec": {
   "display_name": "Python [conda env:dl312] *",
   "language": "python",
   "name": "conda-env-dl312-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
